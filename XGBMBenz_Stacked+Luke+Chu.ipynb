{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "\n",
    "#StackingEstimator = function call\n",
    "#BaseEstimator = All estimators should specify all the parameters that can be set at the class level in their __init__\n",
    "#TransformerMixin = \n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    #Since __init__ is used, StackingEstimator = The data is expected to be stored in a 2D data structure\n",
    "        #, where the first index is over features and the second is over samples. i.e.>> len(data[key]) == n_samples\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "    \n",
    "        #add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        #add class prediction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...   X375  X376  X377  X378  X379  \\\n",
      "0   0  130.81   k  v  at  a  d  u  j  o  ...      0     0     1     0     0   \n",
      "1   6   88.53   k  t  av  e  d  y  l  o  ...      1     0     0     0     0   \n",
      "2   7   76.26  az  w   n  c  d  x  j  x  ...      0     0     0     0     0   \n",
      "3   9   80.62  az  t   n  f  d  x  l  e  ...      0     0     0     0     0   \n",
      "4  13   78.02  az  v   n  f  d  h  d  n  ...      0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 378 columns] (4209, 378) (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "train = pd.read_csv (r\"C:\\Users\\Osula\\Documents\\Projects\\Mercedes-Benz\\train.csv\")\n",
    "test = pd.read_csv (r\"C:\\Users\\Osula\\Documents\\Projects\\Mercedes-Benz\\test.csv\")\n",
    "print(train.head(), train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# find all categorical features\n",
    "cf = train.select_dtypes(include=['object']).columns\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:     ID       y  X0  X1  X2  X3  X4  X5  X6  X8  ...   X375  X376  X377  X378  \\\n",
      "0   0  130.81  37  23  20   0   3  27   9  14  ...      0     0     1     0   \n",
      "1   6   88.53  37  21  22   4   3  31  11  14  ...      1     0     0     0   \n",
      "2   7   76.26  24  24  38   2   3  30   9  23  ...      0     0     0     0   \n",
      "3   9   80.62  24  21  38   5   3  30  11   4  ...      0     0     0     0   \n",
      "4  13   78.02  24  23  38   5   3  14   3  13  ...      0     0     0     0   \n",
      "\n",
      "   X379  X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0  \n",
      "2     0     0     1     0     0     0  \n",
      "3     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 378 columns] (4209, 378)\n",
      "test:     ID  X0  X1  X2  X3  X4  X5  X6  X8  X10  ...   X375  X376  X377  X378  \\\n",
      "0   1  24  23  38   5   3  26   0  22    0  ...      0     0     0     1   \n",
      "1   2  46   3   9   0   3   9   6  24    0  ...      0     0     1     0   \n",
      "2   3  24  23  19   5   3   0   9   9    0  ...      0     0     0     1   \n",
      "3   4  24  13  38   5   3  32  11  13    0  ...      0     0     0     1   \n",
      "4   5  49  20  19   2   3  31   8  12    0  ...      1     0     0     0   \n",
      "\n",
      "   X379  X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 377 columns] (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "#Apply LabelEncoder to categorical features\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "        \n",
    "print('train: ', train.head(), train.shape)\n",
    "print('test: ', test.head(), test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- X0 ----\n",
      "52    360\n",
      "10    349\n",
      "51    324\n",
      "23    313\n",
      "46    306\n",
      "50    300\n",
      "41    269\n",
      "32    227\n",
      "40    195\n",
      "49    182\n",
      "36    181\n",
      "24    175\n",
      "9     151\n",
      "45    106\n",
      "15    103\n",
      "34     75\n",
      "30     73\n",
      "11     67\n",
      "48     36\n",
      "6      35\n",
      "39     34\n",
      "8      34\n",
      "31     32\n",
      "26     27\n",
      "18     25\n",
      "0      21\n",
      "22     19\n",
      "12     18\n",
      "35     18\n",
      "16     18\n",
      "47     17\n",
      "21     16\n",
      "38     16\n",
      "4      14\n",
      "37     11\n",
      "25     11\n",
      "19     11\n",
      "17     10\n",
      "44     10\n",
      "28      6\n",
      "14      4\n",
      "29      3\n",
      "43      2\n",
      "1       2\n",
      "33      1\n",
      "2       1\n",
      "3       1\n",
      "Name: X0, dtype: int64\n",
      "---- X1 ----\n",
      "1     833\n",
      "20    598\n",
      "3     592\n",
      "13    590\n",
      "23    408\n",
      "19    251\n",
      "10    203\n",
      "0     143\n",
      "4     121\n",
      "16     82\n",
      "24     52\n",
      "26     46\n",
      "22     37\n",
      "6      33\n",
      "14     32\n",
      "21     31\n",
      "9      29\n",
      "25     23\n",
      "7      23\n",
      "11     22\n",
      "15     19\n",
      "12     17\n",
      "17      9\n",
      "8       6\n",
      "5       3\n",
      "2       3\n",
      "18      3\n",
      "Name: X1, dtype: int64\n",
      "---- X2 ----\n",
      "19    1659\n",
      "5      496\n",
      "9      415\n",
      "37     367\n",
      "11     265\n",
      "42     153\n",
      "38     137\n",
      "43      94\n",
      "30      87\n",
      "29      81\n",
      "17      63\n",
      "25      54\n",
      "0       47\n",
      "44      29\n",
      "35      25\n",
      "33      25\n",
      "26      21\n",
      "15      20\n",
      "7       19\n",
      "49      19\n",
      "28      18\n",
      "3       13\n",
      "31      12\n",
      "48      11\n",
      "16      11\n",
      "47      10\n",
      "23       8\n",
      "20       6\n",
      "32       6\n",
      "41       5\n",
      "14       5\n",
      "12       5\n",
      "8        4\n",
      "40       4\n",
      "22       4\n",
      "21       3\n",
      "34       1\n",
      "36       1\n",
      "1        1\n",
      "27       1\n",
      "39       1\n",
      "18       1\n",
      "13       1\n",
      "6        1\n",
      "Name: X2, dtype: int64\n",
      "---- X3 ----\n",
      "2    1942\n",
      "5    1076\n",
      "0     440\n",
      "3     290\n",
      "6     241\n",
      "4     163\n",
      "1      57\n",
      "Name: X3, dtype: int64\n",
      "---- X4 ----\n",
      "3    4205\n",
      "0       2\n",
      "2       1\n",
      "1       1\n",
      "Name: X4, dtype: int64\n",
      "---- X5 ----\n",
      "28    231\n",
      "29    231\n",
      "23    220\n",
      "24    215\n",
      "25    214\n",
      "11    214\n",
      "20    212\n",
      "19    208\n",
      "22    208\n",
      "15    207\n",
      "5     205\n",
      "7     204\n",
      "3     200\n",
      "2     197\n",
      "18    195\n",
      "6     188\n",
      "4     185\n",
      "17    177\n",
      "10    131\n",
      "16    125\n",
      "1     112\n",
      "8      97\n",
      "21     20\n",
      "12      7\n",
      "30      2\n",
      "13      1\n",
      "27      1\n",
      "14      1\n",
      "31      1\n",
      "Name: X5, dtype: int64\n",
      "---- X6 ----\n",
      "6     1042\n",
      "9     1039\n",
      "3      625\n",
      "8      488\n",
      "11     478\n",
      "0      206\n",
      "7      190\n",
      "10      43\n",
      "2       38\n",
      "1       28\n",
      "5       20\n",
      "4       12\n",
      "Name: X6, dtype: int64\n",
      "---- X8 ----\n",
      "9     277\n",
      "18    255\n",
      "5     243\n",
      "13    242\n",
      "8     237\n",
      "4     225\n",
      "17    219\n",
      "0     210\n",
      "22    196\n",
      "21    194\n",
      "1     190\n",
      "10    176\n",
      "14    163\n",
      "12    155\n",
      "6     130\n",
      "19    119\n",
      "20    119\n",
      "7     117\n",
      "16    117\n",
      "24    116\n",
      "23    105\n",
      "3     103\n",
      "11    101\n",
      "2     100\n",
      "15    100\n",
      "Name: X8, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for levels in train[cf].columns:\n",
    "    print (\"---- %s ----\" % levels)\n",
    "    print (train[levels].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347']\n"
     ]
    }
   ],
   "source": [
    "## drop columns with only one value\n",
    "k = train.loc[:,(train.apply(pd.Series.nunique) == 1)].columns.tolist()\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_drop = train.drop(k, axis = 1)\n",
    "test_drop = test.drop(k, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X290', 'X330', 'X187', 'X323', 'X209', 'X256', 'X125', 'X224', 'X26', 'X152', 'X139', 'X37', 'X44', 'X147', 'X11', 'X231', 'X301', 'X237', 'X165', 'X366', 'X196', 'X340', 'X200', 'X279', 'X385', 'X122', 'X167', 'X228', 'X274', 'X371', 'X91', 'X159', 'X251', 'X281', 'X84', 'X318', 'X184', 'X14', 'X273', 'X183', 'X331', 'X189', 'X40', 'X150', 'X246', 'X336', 'X16', 'X4', 'X17', 'X46', 'X223', 'X247', 'X284', 'X363', 'X73', 'X195', 'X204', 'X162', 'X173', 'X21', 'X257', 'X82', 'X357', 'X212', 'X156', 'X169', 'X270', 'X378', 'X232', 'X74', 'X78', 'X203', 'X110', 'X131', 'X182', 'X27', 'X218', 'X297', 'X311', 'X133', 'X118', 'X24', 'X320', 'ID', 'X52', 'X382', 'X42', 'X98', 'X230', 'X123', 'X288', 'X197', 'X32', 'X277', 'X179', 'X59', 'X135', 'X53', 'X265', 'X263', 'X130', 'X113', 'X63', 'X250', 'X18', 'X164', 'X208', 'X56', 'X99', 'X226', 'X337', 'X344', 'X329', 'X291', 'X34', 'X317', 'X264', 'X254', 'X142', 'X35', 'X127', 'X117', 'X275', 'X328', 'X260', 'X45', 'X170', 'X276', 'X77', 'X1', 'X343', 'X283', 'X355', 'X347', 'X351', 'X333', 'X216', 'X48', 'X50', 'X259', 'X253', 'X233', 'X51', 'X85', 'X86', 'X367', 'X47', 'X299', 'X100', 'X191', 'X348', 'X211', 'X87', 'X104', 'X175', 'X2', 'X90', 'X282', 'X312', 'X33', 'X217', 'X83', 'X370', 'X29', 'X313', 'X342', 'X49', 'X103', 'X93', 'X89', 'X241', 'X267', 'X79', 'X112', 'X213', 'X57', 'X194', 'X185', 'X306', 'X359', 'X239', 'X5', 'X307', 'X88', 'X364', 'X271', 'X205', 'X41', 'X39', 'X134', 'X120', 'X190', 'X38', 'X206', 'X266', 'X335', 'X341', 'X144', 'X111', 'X215', 'X69', 'X36', 'X255', 'X296', 'X234', 'X114', 'X115', 'X160', 'X146', 'X207', 'X154', 'X30', 'X138', 'X326', 'X3', 'X379', 'X244', 'X316', 'X28', 'X238', 'X334', 'X372', 'X145', 'X210', 'X268', 'X380', 'X20', 'X287', 'X168', 'X96', 'X171', 'X285', 'X362', 'X65', 'X61', 'X325', 'X8', 'X58', 'X201', 'X81', 'X322', 'X375', 'X76', 'X13', 'X192', 'X269', 'X374', 'X68', 'X292', 'X310', 'X321', 'X64', 'X126', 'X236', 'X369', 'X304', 'X12', 'X140', 'X23', 'X43', 'X177', 'X360', 'X309', 'X302', 'X221', 'X368', 'X338', 'X352', 'X361', 'X124', 'X161', 'X202', 'X136', 'X294', 'X300', 'X198', 'X354', 'X356', 'X384', 'X327', 'X376', 'X95', 'X137', 'X180', 'X298', 'X67', 'X66', 'X353', 'X248', 'X172', 'X249', 'X158', 'X225', 'X214', 'X219', 'X186', 'X252', 'X295', 'X349', 'X272', 'X305', 'X132', 'X289', 'X143', 'X102', 'X377', 'X101', 'X163', 'X240', 'X315', 'X22', 'X31', 'X92', 'X222', 'X235', 'X15', 'X278', 'X105', 'X220', 'X358', 'X293', 'X108', 'X350', 'X286', 'X280', 'X80', 'X60', 'X174', 'X332', 'X116', 'X71', 'X6', 'X319', 'X262', 'X227', 'X229', 'X181', 'X148', 'X106', 'X155', 'X339', 'X166', 'X308', 'X109', 'X55', 'X345', 'X10', 'X151', 'X62', 'X75', 'X258', 'X128', 'X199', 'X70', 'X346', 'X54', 'X178', 'X261', 'X153', 'X383', 'X243', 'X157', 'X245', 'X107', 'X94', 'X176', 'X19', 'X119', 'X242', 'X324', 'X365', 'X97', 'X0', 'X141', 'X314', 'X373', 'X129']\n"
     ]
    }
   ],
   "source": [
    "#save columns list before adding the decomposition components\n",
    "#list = Converts a tuple/sequence into list.\n",
    "#set = creating a list that ignores all duplicates\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "print(usable_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "**Linear dimensionality reduction** using **Singular Value Decomposition** of the data to project it to a lower dimensional space.\n",
    "\n",
    "It uses the **LAPACK** implementation of the **_full SVD_** or a **_randomized truncated SVD_** by the method of Halko et al. 2009, depending on the *shape of the input data* and the *number of components* to extract.\n",
    "\n",
    "It can also use the **scipy.sparse.linalg ARPACK** implementation of the **_truncated SVD_**.\n",
    "\n",
    "Notice that this class **_does not_** support **sparse input**. See **_TruncatedSVD_** for an alternative with sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [[ -4.20591806e+03   3.74113607e-03  -3.95586587e-02 ...,   4.10594534e+00\n",
      "    1.64261012e+00  -4.98593291e-01]\n",
      " [ -4.19990961e+03  -5.32835499e-02   1.78049019e+00 ...,  -4.77141762e-01\n",
      "    9.30734173e-01  -6.26763229e-01]\n",
      " [ -4.19891168e+03   1.64742769e+01   1.38064828e+01 ...,   1.17196099e+00\n",
      "    1.70121051e+00  -4.36638237e-01]\n",
      " ..., \n",
      " [  4.20599752e+03   3.08713176e+01   1.60994784e+01 ...,   3.14349853e+00\n",
      "   -3.35521928e-01   1.04878608e-01]\n",
      " [  4.20899773e+03   2.54358916e+01   2.87739952e+00 ...,  -3.00067866e+00\n",
      "    1.61686969e+00   2.07532238e+00]\n",
      " [  4.21099776e+03  -1.90654871e+01  -1.08794911e+01 ...,   9.30625134e-01\n",
      "    6.80522968e-01   6.37402084e-01]] (4209, 10)\n",
      "test:  [[ -4.20492117e+03   1.64075037e+01   1.36275938e+01 ...,  -1.62546766e+00\n",
      "    2.41023593e+00   1.39799383e+00]\n",
      " [ -4.20396516e+03  -1.54761828e+01  -9.29573483e+00 ...,   4.02584553e+00\n",
      "    1.99656288e+00  -4.20565776e-01]\n",
      " [ -4.20298015e+03   1.27465266e+01  -4.12623303e+00 ...,  -9.94777470e-01\n",
      "    4.82851005e-01   3.86352302e-01]\n",
      " ..., \n",
      " [  4.20699748e+03  -1.39117974e+01   2.34392817e+00 ...,  -2.60397702e+00\n",
      "   -1.47823044e+00   2.57171877e+00]\n",
      " [  4.20800025e+03   2.52154268e+01  -6.95349509e+00 ...,   3.61165573e+00\n",
      "   -2.12917697e+00  -2.42960452e-02]\n",
      " [  4.20999219e+03  -1.60969769e+01  -8.10008446e+00 ...,   1.13162895e-01\n",
      "    1.16140851e+00  -1.79498752e+00]] (4209, 10)\n"
     ]
    }
   ],
   "source": [
    "## Adding in PCA, FA, etc.\n",
    "from sklearn.decomposition import PCA\n",
    "n_comp = 10 #Number of components to keep, if not set all components are kept: n_components == min(n_samples, n_features)\n",
    "                #if n_components == ‘mle’ and svd_solver == ‘full’\n",
    "                #, Minka’s MLE is used to guess the dimension if 0 < n_components < 1 and svd_solver == ‘full’\n",
    "                #, select the number of components such that the amount of variance that needs to be explained \n",
    "                #is greater than the percentage specified by n_components n_components cannot be equal to n_features \n",
    "                #for svd_solver == ‘arpack’.\n",
    "                \n",
    "r_state = 2017 #random_state = Pseudo Random Number generator seed control. If None, use the numpy.random singleton. \n",
    "                    #Used by svd_solver == ‘arpack’ or ‘randomized’.\n",
    "    \n",
    "pca = PCA(n_components=n_comp, random_state = r_state)\n",
    "pca2_results_train = pca.fit_transform(train_drop.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test_drop)\n",
    "print('train: ', pca2_results_train, pca2_results_train.shape)\n",
    "print('test: ', pca2_results_test, pca2_results_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4205.91805677 -4199.90960823 -4198.91167793 ...,  4205.99752281\n",
      "  4208.99773408  4210.99776075]\n",
      "[  3.74113607e-03  -5.32835499e-02   1.64742769e+01 ...,   3.08713176e+01\n",
      "   2.54358916e+01  -1.90654871e+01]\n",
      "[ -0.03955866   1.78049019  13.80648279 ...,  16.09947845   2.87739952\n",
      " -10.87949109]\n",
      "[ 13.23697213  11.42255123  11.67891572 ...,   3.88662363   1.88931611\n",
      "  11.50243592]\n",
      "[ -4.33478539  -5.08774191 -15.07352322 ...,   9.27298592  -6.05284658\n",
      "  -5.08192779]\n",
      "[-21.25489826 -25.18849186 -23.0570205  ...,  22.51485373  24.59409031\n",
      "  25.05360854]\n",
      "[-2.75236339 -4.5074752  -2.23281044 ...,  0.81964321 -4.29653933\n",
      "  0.57595582]\n",
      "[ 4.10594534 -0.47714176  1.17196099 ...,  3.14349853 -3.00067866\n",
      "  0.93062513]\n",
      "[ 1.64261012  0.93073417  1.70121051 ..., -0.33552193  1.61686969\n",
      "  0.68052297]\n",
      "[-0.49859329 -0.62676323 -0.43663824 ...,  0.10487861  2.07532238\n",
      "  0.63740208]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, n_comp):\n",
    "    print(pca2_results_train[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [[ 1.01752745  0.32106122  0.24191141 ..., -0.26241573  8.68341608\n",
      "   1.86284317]\n",
      " [ 0.54053354  1.18222149  0.01315693 ..., -0.12814816 -0.22289462\n",
      "   1.71872336]\n",
      " [ 0.932777    2.19708049  0.73814656 ...,  0.03552488  0.1989201\n",
      "   1.75451444]\n",
      " ..., \n",
      " [ 0.54053271  1.18222281  0.01315536 ..., -0.12814629 -0.22285169\n",
      "  -1.73564192]\n",
      " [-0.77868697  0.58024745 -1.1395795  ...,  0.04310846 -0.01572709\n",
      "  -1.66080455]\n",
      " [-0.77868697  0.58024745 -1.1395795  ...,  0.04310846 -0.01572708\n",
      "  -1.66162654]]\n",
      "test:  [[ 0.932777    2.19708049  0.73814656 ...,  0.03552488  0.19892007\n",
      "   1.75698017]\n",
      " [-1.63626944 -0.02871052  1.34605866 ...,  0.15177823  0.0779865\n",
      "   1.72778262]\n",
      " [ 1.69574441  0.32042195  0.67617552 ..., -0.14550339  0.23878725\n",
      "   1.69383851]\n",
      " ..., \n",
      " [-0.01571956 -1.29641109 -1.20155054 ..., -0.1379198   0.02414007\n",
      "  -1.72230396]\n",
      " [ 1.30350012 -0.69443573 -0.04881568 ..., -0.30917455 -0.18298451\n",
      "  -1.79878417]\n",
      " [-0.77868697  0.58024745 -1.1395795  ...,  0.04310846 -0.01572709\n",
      "  -1.6612152 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "FA = FactorAnalysis(n_components=n_comp, random_state = r_state)\n",
    "FA_results_train = FA.fit_transform(train_drop.drop([\"y\"], axis=1))\n",
    "FA_results_test = FA.transform(test_drop)\n",
    "print('train: ', FA_results_train)\n",
    "print('test: ', FA_results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Singular Value Decomposition (SVD)\n",
    "Since our data input is spare, we try the Truncated SVD as a good alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The **TruncatedSVD** transformer performs *linear dimensionality reduction* by means of **truncated singular value decomposition (SVD)**. Contrary to *PCA*, this estimator **_does not_** center the data before computing the singular value decomposition. This means it **_can work_** with **scipy.sparse matrices** efficiently.\n",
    "\n",
    "In particular, truncated SVD **_works on_** term *count/tf-idf matrices* as returned by the vectorizers in **sklearn.feature_extraction.text**. In that context, it is known as **latent semantic analysis (LSA)**.\n",
    "\n",
    "This estimator **_supports_** *two algorithms*: a fast **randomized SVD solver**, and a *“naive” algorithm* that uses **ARPACK** as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state = r_state)\n",
    "tsvd_results_train = tsvd.fit_transform(train_drop.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis (ICA)\n",
    "**FastICA**: a fast algorithm for Independent Component Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Independent component analysis** separates a *multivariate signal* into *additive subcomponents* that are **_maximally independent_**. It is implemented in scikit-learn using the **Fast ICA** algorithm. Typically, ICA is **_not used_** for *reducing dimensionality* but *__for separating superimposed signals__*. \n",
    "\n",
    "Since the ICA model __*does not*__ include a noise term, for the model to be correct, whitening **_must_** be applied. This can be done internally using the **whiten argument** or manually using one of the **PCA variants**.\n",
    "\n",
    "It is classically used to *separate mixed signals* (a problem known as *blind source separation*) and can also be used as yet another *non linear decomposition* that finds components with **_some sparsity_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state = r_state)\n",
    "ica2_results_train = ica.fit_transform(train_drop.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00385651 -0.01110805 -0.05618819 ...,  0.00307634 -0.00470934\n",
      "  -0.01422798]\n",
      " [ 0.00909563 -0.00259149 -0.06391768 ...,  0.00133614 -0.00620145\n",
      "  -0.02523413]\n",
      " [ 0.00784402  0.00400473 -0.06131861 ..., -0.0139092  -0.02396659\n",
      "  -0.01549358]\n",
      " ..., \n",
      " [-0.00869924  0.0180974   0.05986589 ..., -0.00461554  0.0182967\n",
      "   0.00522132]\n",
      " [-0.0086041  -0.00609768  0.0599258  ...,  0.01094057 -0.01884846\n",
      "  -0.01673516]\n",
      " [-0.00746153 -0.02025349  0.06081357 ...,  0.01074364 -0.02179523\n",
      "   0.0071692 ]] (4209, 10)\n"
     ]
    }
   ],
   "source": [
    "print(ica2_results_train, ica2_results_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00385651  0.00909563  0.00784402 ..., -0.00869924 -0.0086041\n",
      " -0.00746153]\n",
      "[-0.01110805 -0.00259149  0.00400473 ...,  0.0180974  -0.00609768\n",
      " -0.02025349]\n",
      "[-0.05618819 -0.06391768 -0.06131861 ...,  0.05986589  0.0599258\n",
      "  0.06081357]\n",
      "[ 0.00768804  0.00711497 -0.00637006 ..., -0.02063642 -0.02786758\n",
      "  0.01874777]\n",
      "[ 0.03317202 -0.00065743  0.01162993 ...,  0.02376639 -0.02780403\n",
      "  0.00559164]\n",
      "[-0.01560285 -0.00717329 -0.02654893 ..., -0.01562375 -0.02403196\n",
      " -0.00111385]\n",
      "[ 0.02022935  0.02037331  0.02278592 ...,  0.01579916  0.01044916\n",
      "  0.01238623]\n",
      "[ 0.00307634  0.00133614 -0.0139092  ..., -0.00461554  0.01094057\n",
      "  0.01074364]\n",
      "[-0.00470934 -0.00620145 -0.02396659 ...,  0.0182967  -0.01884846\n",
      " -0.02179523]\n",
      "[-0.01422798 -0.02523413 -0.01549358 ...,  0.00522132 -0.01673516\n",
      "  0.0071692 ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, n_comp):\n",
    "    print(ica2_results_train[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Random Projection (GRP)\n",
    "*Reduce dimensionality* through Gaussian random projection: the components of the *random matrix* are drawn from \n",
    "**N(0, 1 / n_components)**.\n",
    "\n",
    "The **sklearn.random_projection.GaussianRandomProjection** reduces the dimensionality by projecting the original input space on a *randomly generated matrix* where components are drawn from the following **distribution N(0, 1\\n_components)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRP\n",
    "#esp = strictly positive float, optional (default=0.1). Parameter to control the quality of the embedding according to the \n",
    "    #Johnson-Lindenstrauss lemma when n_components is set to ‘auto’. \n",
    "    #Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space.\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state = r_state)\n",
    "grp_results_train = grp.fit_transform(train_drop.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Random Projection (SRP)\n",
    "*Reduce dimensionality* through sparse random projection\n",
    "\n",
    "**Sparse random matrix**s an alternative to *dense random projection matrix* that guarantees similar embedding quality while being much more __*memory efficient*__ and allowing **_faster computation_** of the projected data.\n",
    "\n",
    "If we note **s = 1 / density** the components of the *random matrix* are drawn from:\n",
    "- **-sqrt(s) / sqrt(n_components)** with probability **1 / 2s**\n",
    "- 0 with probability **1 - 1 / s**\n",
    "- **+sqrt(s) / sqrt(n_components)** with probability **1 / 2s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state = r_state)\n",
    "srp_results_train = srp.fit_transform(train_drop.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append decomposition components to datasets\n",
    "for i in range(0, n_comp):\n",
    "    train_drop['pca_' + str(i)] = pca2_results_train[:,i]\n",
    "    test_drop['pca_' + str(i)] = pca2_results_test[:, i]\n",
    "    \n",
    "    #train_drop['fa' + str(i)] = FA_results_train[:,i-1]\n",
    "    #test_drop['fa' + str(i)] = FA_results_test[:, i-1]\n",
    "\n",
    "    train_drop['ica_' + str(i)] = ica2_results_train[:,i]\n",
    "    test_drop['ica_' + str(i)] = ica2_results_test[:, i]\n",
    "\n",
    "    #train_drop['tsvd_' + str(i)] = tsvd_results_train[:,i-1]\n",
    "    #test_drop['tsvd_' + str(i)] = tsvd_results_test[:, i-1]\n",
    "\n",
    "    #train_drop['grp_' + str(i)] = grp_results_train[:,i-1]\n",
    "    #test_drop['grp_' + str(i)] = grp_results_test[:, i-1]\n",
    "\n",
    "    #train_drop['srp_' + str(i)] = srp_results_train[:,i-1]\n",
    "    #test_drop['srp_' + str(i)] = srp_results_test[:, i-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>ica_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>ica_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>ica_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>ica_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>ica_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.254898</td>\n",
       "      <td>-0.015603</td>\n",
       "      <td>-2.752363</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>4.105945</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>1.642610</td>\n",
       "      <td>-0.004709</td>\n",
       "      <td>-0.498593</td>\n",
       "      <td>-0.014228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.188492</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-4.507475</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>-0.477142</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.930734</td>\n",
       "      <td>-0.006201</td>\n",
       "      <td>-0.626763</td>\n",
       "      <td>-0.025234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.057021</td>\n",
       "      <td>-0.026549</td>\n",
       "      <td>-2.232810</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>1.171961</td>\n",
       "      <td>-0.013909</td>\n",
       "      <td>1.701211</td>\n",
       "      <td>-0.023967</td>\n",
       "      <td>-0.436638</td>\n",
       "      <td>-0.015494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.484677</td>\n",
       "      <td>-0.027332</td>\n",
       "      <td>-4.362057</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>-1.888300</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>2.218975</td>\n",
       "      <td>0.016281</td>\n",
       "      <td>0.212048</td>\n",
       "      <td>-0.025615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.548080</td>\n",
       "      <td>-0.032017</td>\n",
       "      <td>3.716605</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>-1.741610</td>\n",
       "      <td>-0.014339</td>\n",
       "      <td>2.174680</td>\n",
       "      <td>-0.001974</td>\n",
       "      <td>1.282100</td>\n",
       "      <td>0.017934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X0  X1  X2  X3  X4  X5  X6  X8    ...         pca_5     ica_5  \\\n",
       "0   0  130.81  37  23  20   0   3  27   9  14    ...    -21.254898 -0.015603   \n",
       "1   6   88.53  37  21  22   4   3  31  11  14    ...    -25.188492 -0.007173   \n",
       "2   7   76.26  24  24  38   2   3  30   9  23    ...    -23.057021 -0.026549   \n",
       "3   9   80.62  24  21  38   5   3  30  11   4    ...    -25.484677 -0.027332   \n",
       "4  13   78.02  24  23  38   5   3  14   3  13    ...     -8.548080 -0.032017   \n",
       "\n",
       "      pca_6     ica_6     pca_7     ica_7     pca_8     ica_8     pca_9  \\\n",
       "0 -2.752363  0.020229  4.105945  0.003076  1.642610 -0.004709 -0.498593   \n",
       "1 -4.507475  0.020373 -0.477142  0.001336  0.930734 -0.006201 -0.626763   \n",
       "2 -2.232810  0.022786  1.171961 -0.013909  1.701211 -0.023967 -0.436638   \n",
       "3 -4.362057  0.019089 -1.888300 -0.004061  2.218975  0.016281  0.212048   \n",
       "4  3.716605  0.020845 -1.741610 -0.014339  2.174680 -0.001974  1.282100   \n",
       "\n",
       "      ica_9  \n",
       "0 -0.014228  \n",
       "1 -0.025234  \n",
       "2 -0.015494  \n",
       "3 -0.025615  \n",
       "4  0.017934  \n",
       "\n",
       "[5 rows x 386 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    130.81\n",
       "1     88.53\n",
       "2     76.26\n",
       "3     80.62\n",
       "4     78.02\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## My response variables\n",
    "y_train = train[\"y\"]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  100.66931812782121\n",
      "median:  99.15\n"
     ]
    }
   ],
   "source": [
    "## My baseline prediction: an average of the y-values\n",
    "y_mean = np.mean(y_train)\n",
    "y_median = np.median(y_train)\n",
    "print('mean: ', y_mean)\n",
    "print('median: ', y_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_test:  [   1    2    3 ..., 8413 8414 8416]\n",
      "train:  [[0 0 1 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 1 ..., 1 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "id_test = test['ID'].values\n",
    "print('id_test: ', id_test)\n",
    "\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "print('train: ', finaltrainset, finaltrainset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train the xgb model then predict the test data'''\n",
    "\n",
    "# Prepare dict of params for XGBoost to run with\n",
    "xgb_params = {\n",
    "    ## Number of Trees\n",
    "    'n_trees': 395, \n",
    "    ## Learning Rate; default = 0.3\n",
    "    'eta': 0.0065,\n",
    "    ## Depth of Trees\n",
    "    'max_depth': 3,\n",
    "    ## Bagging 50% of the training set\n",
    "    'subsample': 0.50,\n",
    "    #'colsample_bytree': 0.75,\n",
    "    'min_child_weight': 34,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    # Base Prediction = mean(target)\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.6296\ttest-rmse:12.6244\n",
      "[50]\ttrain-rmse:10.7971\ttest-rmse:10.7987\n",
      "[100]\ttrain-rmse:9.69451\ttest-rmse:9.70491\n",
      "[150]\ttrain-rmse:9.05374\ttest-rmse:9.07756\n",
      "[200]\ttrain-rmse:8.6935\ttest-rmse:8.73086\n",
      "[250]\ttrain-rmse:8.49311\ttest-rmse:8.54569\n",
      "[300]\ttrain-rmse:8.37528\ttest-rmse:8.44598\n",
      "[350]\ttrain-rmse:8.30265\ttest-rmse:8.39152\n",
      "[400]\ttrain-rmse:8.25229\ttest-rmse:8.36468\n",
      "[450]\ttrain-rmse:8.21382\ttest-rmse:8.35007\n",
      "[500]\ttrain-rmse:8.18218\ttest-rmse:8.34125\n",
      "[550]\ttrain-rmse:8.15497\ttest-rmse:8.33481\n",
      "[600]\ttrain-rmse:8.12776\ttest-rmse:8.33175\n"
     ]
    }
   ],
   "source": [
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1500,\n",
    "                   #nfold=10,\n",
    "                   early_stopping_rounds=25,\n",
    "                   verbose_eval=50, \n",
    "                   show_stdv=False,\n",
    "                   seed=2017\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_boost_rounds = len(cv_result)\n",
    "num_boost_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train the stacked models then predict the test data'''\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(\n",
    "        learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55,\n",
    "        min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.081e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.772e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.772e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.506e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.506e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.376e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.376e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.351e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.351e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.350e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=1.356e-02, previous alpha=1.350e-02, with an active set of 18 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.771e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.483e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.483e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.065e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.065e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.065e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=8.754e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.201e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.201e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.201e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=6.370e-03, previous alpha=5.807e-03, with an active set of 34 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.688e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.181e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.846e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.804e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.535e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.369e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.267e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=1.317e-02, previous alpha=1.180e-02, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.543e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.301e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.301e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.285e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.367e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.367e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.365e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.250e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.212e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.212e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.194e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.121e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.247e-02, previous alpha=1.115e-02, with an active set of 19 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.368e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.205e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.008e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.007e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.663e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.655e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.162e-03, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.162e-03, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.089e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.089e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.089e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 82 iterations, alpha=2.072e-03, previous alpha=2.068e-03, with an active set of 77 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.581e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.041e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.354e-03, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.069e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.069e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.069e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.069e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.012e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.012e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.980e-03, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.693e-03, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.693e-03, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.535e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 94 iterations, alpha=1.562e-03, previous alpha=1.513e-03, with an active set of 93 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.790e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.690e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.690e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.750e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.393e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.393e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.393e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Osula\\Anaconda2\\envs\\python35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=3.324e-03, previous alpha=3.306e-03, with an active set of 41 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "results = stacked_pipeline.predict(finaltestset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score on Training Set:\n",
      "0.583184414444\n",
      "Time Elapsed: 26.8678035736084\n"
     ]
    }
   ],
   "source": [
    "'''R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('R2 Score on Training Set:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "print('Time Elapsed: {}'.format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('xgboost_Stacked2_Fudged.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
